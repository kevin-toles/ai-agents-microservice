# Code Understanding Orchestrator Service

> ## âš ï¸ DEPRECATED
> **Date**: January 2025  
> **Reason**: This document is a duplicate. The canonical version lives in the Code-Orchestrator-Service repository.
> 
> **Canonical Document**:
> - [Code-Orchestrator-Service/docs/ARCHITECTURE.md](/Code-Orchestrator-Service/docs/ARCHITECTURE.md) - Sous Chef architecture
>
> **Platform Context**:
> - [AI_CODING_PLATFORM_ARCHITECTURE.md](/textbooks/pending/platform/AI_CODING_PLATFORM_ARCHITECTURE.md) - Kitchen Brigade overview
>
> This document is retained for historical reference only. Do not update.

## Executive Summary

A standalone microservice that coordinates multiple specialized code understanding models (CodeT5+, GraphCodeBERT, CodeBERT) to dynamically extract, validate, and rank search terms from natural language queries. This service replaces hardcoded keyword mappings with intelligent, context-aware term generation.

This service acts as the **"Sous Chef"** in the Kitchen Brigade architectureâ€”interpreting orders (queries), preparing ingredients (keywords), curating results, and auditing output before serving to the customer.

---

## Kitchen Brigade Architecture Model

### The Analogy

The platform follows a **Kitchen Brigade** organizational model where each service has a specific role:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ðŸ½ï¸  KITCHEN BRIGADE MODEL                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  ðŸ‘¤ CUSTOMER (Claude/GPT/User)                                              â”‚
â”‚     â””â”€â†’ Places order: "I need code for document chunking with overlap"      â”‚
â”‚                                                                              â”‚
â”‚  ðŸ‘¨â€ðŸ³ SOUS CHEF (Code Understanding Orchestrator) â† THIS SERVICE             â”‚
â”‚     â””â”€â†’ SMART: Interprets the order                                         â”‚
â”‚     â””â”€â†’ Extracts keywords/concepts using code understanding models          â”‚
â”‚     â””â”€â†’ Sends keyword list to Cookbook                                      â”‚
â”‚                                                                              â”‚
â”‚  ðŸ“– COOKBOOK (Semantic Search Service) â† DUMB RETRIEVAL                     â”‚
â”‚     â””â”€â†’ Takes keywords as INPUT (does NOT generate them)                    â”‚
â”‚     â””â”€â†’ Queries vector DBs (Qdrant, Neo4j) where content lives              â”‚
â”‚     â””â”€â†’ Returns ALL matches without filtering or judgment                   â”‚
â”‚     â””â”€â†’ Just a retrieval engine - like looking up recipes in a book         â”‚
â”‚                                                                              â”‚
â”‚  ðŸ‘¨â€ðŸ³ CHEF DE PARTIE (Orchestrator - Curation Phase)                         â”‚
â”‚     â””â”€â†’ Receives raw results from Cookbook                                  â”‚
â”‚     â””â”€â†’ SMART: Filters out irrelevant results (C++ "chunk of memory")       â”‚
â”‚     â””â”€â†’ Ranks by domain relevance                                           â”‚
â”‚     â””â”€â†’ Prepares curated instructions for Line Cook                         â”‚
â”‚                                                                              â”‚
â”‚  ðŸ‘¨â€ðŸ³ LINE COOK (Code Llama via LLM Gateway)                                 â”‚
â”‚     â””â”€â†’ Receives curated context + instructions                             â”‚
â”‚     â””â”€â†’ Generates actual code from the instructions                         â”‚
â”‚                                                                              â”‚
â”‚  ðŸ‘¨â€ðŸ³ CHEF DE PARTIE (Orchestrator - Audit Phase)                            â”‚
â”‚     â””â”€â†’ Validates generated code quality                                    â”‚
â”‚     â””â”€â†’ Ensures code matches original intent                                â”‚
â”‚                                                                              â”‚
â”‚  ðŸ‘¤ CUSTOMER receives the final plated dish (working code)                  â”‚
â”‚     â””â”€â†’ Implements the code in their project                                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Responsibility Matrix

| Service | Role | Intelligence | What It Does | What It Does NOT Do |
|---------|------|--------------|--------------|---------------------|
| **LLM Gateway** | Router | Routing only | Routes requests to appropriate models | Make decisions about content |
| **Code Understanding Orchestrator** | Sous Chef + Chef de Partie | **SMART** | Extracts keywords, curates results, audits output | Store content, execute searches |
| **Semantic Search Service** | Cookbook | **DUMB** | Takes keywords as input, queries vector DBs, returns all matches | Generate keywords, filter results, make judgments |
| **Code Llama** | Line Cook | Executor | Generates code from curated instructions | Decide what to generate |
| **Vector DBs (Qdrant/Neo4j)** | Pantry | Storage | Stores embeddings and relationships | Nothing else |

### Key Insight: Semantic Search is DUMB

The **Semantic Search Service** is intentionally dumb:
- It does NOT contain knowledge itselfâ€”it queries databases that contain knowledge
- It does NOT generate keywordsâ€”it receives them as input
- It does NOT filter resultsâ€”it returns ALL matches
- It's just a query executor, like looking up recipes in a cookbook

The **intelligence lives in the Orchestrator**, which:
1. **Interprets** the customer's order (query understanding)
2. **Generates** the right keywords to search for
3. **Curates** the raw results (filters irrelevant matches)
4. **Instructs** the line cook (prepares context for code generation)
5. **Audits** the final output (validates generated code)

---

## Problem Statement

### Current State
The existing cross-reference system uses **hardcoded `FOCUS_SEARCH_TERMS`** mappings:

```python
FOCUS_SEARCH_TERMS = {
    "multi-stage chunking": [
        "chunk", "chunking", "split", "segment", ...  # Static, brittle
    ],
}
```

### Issues
1. **False Positives**: "chunk" matches C++ memory allocation ("chunk of memory") instead of LLM document chunking
2. **Not Portable**: Hardcoded terms don't transfer across taxonomies/domains
3. **Maintenance Burden**: Manual updates required for new concepts
4. **Limited Coverage**: Misses semantically related terms not in the list

### Proposed Solution
A multi-model orchestration service that dynamically generates contextually-relevant search terms.

---

## Architecture Overview

### High-Level System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ðŸ‘¤ CUSTOMER (Claude/GPT/User)                            â”‚
â”‚                "I need code for document chunking with overlap"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Request
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ðŸ‘¨â€ðŸ³ CODE UNDERSTANDING ORCHESTRATOR (Sous Chef)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         API Gateway                                    â”‚  â”‚
â”‚  â”‚                    /extract, /validate, /search                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                  â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      Agent Orchestrator                                â”‚  â”‚
â”‚  â”‚                   (LangGraph State Machine)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â”‚                       â”‚                       â”‚                       â”‚
â”‚      â–¼                       â–¼                       â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  CodeT5+    â”‚       â”‚GraphCodeBERTâ”‚       â”‚  CodeBERT   â”‚                â”‚
â”‚  â”‚  Agent      â”‚       â”‚   Agent     â”‚       â”‚   Agent     â”‚                â”‚
â”‚  â”‚ (Generator) â”‚       â”‚ (Validator) â”‚       â”‚  (Ranker)   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                              â”‚
â”‚  Output: ["chunking", "text_splitter", "overlap", "RAG", "embedding"]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Keywords (INPUT to Cookbook)
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ðŸ“– SEMANTIC SEARCH SERVICE (Cookbook) - DUMB                  â”‚
â”‚                                                                              â”‚
â”‚  Input:  Keywords from Orchestrator                                          â”‚
â”‚  Action: Query vector databases                                              â”‚
â”‚  Output: ALL matches (no filtering, no judgment)                            â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Qdrant        â”‚  â”‚   Neo4j Graph   â”‚  â”‚   Hybrid        â”‚             â”‚
â”‚  â”‚   Retriever     â”‚  â”‚   Retriever     â”‚  â”‚   Search        â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                    â”‚                    â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                â”‚                                             â”‚
â”‚           Returns: [C++ memory chunk, LLM chunking, game chunks, ...]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Raw Results
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ðŸ‘¨â€ðŸ³ ORCHESTRATOR (Chef de Partie) - Curation Phase                 â”‚
â”‚                                                                              â”‚
â”‚  âœ“ Filter: Remove C++ "chunk of memory" (wrong domain)                      â”‚
â”‚  âœ“ Rank: Score by relevance to LLM/AI context                               â”‚
â”‚  âœ“ Prepare: Curated context for Line Cook                                   â”‚
â”‚                                                                              â”‚
â”‚  Output: Curated references + instructions for code generation              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Curated Context
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ‘¨â€ðŸ³ LINE COOK (Code Llama via LLM Gateway)                â”‚
â”‚                                                                              â”‚
â”‚  Input:  Curated context + generation instructions                          â”‚
â”‚  Action: Generate code based on best practices from references              â”‚
â”‚  Output: Working code implementation                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Generated Code
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ðŸ‘¨â€ðŸ³ ORCHESTRATOR (Chef de Partie) - Audit Phase                   â”‚
â”‚                                                                              â”‚
â”‚  âœ“ Validate: Code quality checks                                            â”‚
â”‚  âœ“ Verify: Matches original intent                                          â”‚
â”‚  âœ“ Format: Prepare final output                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ Final Result
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ðŸ‘¤ CUSTOMER receives final dish                      â”‚
â”‚                      (Working code ready to implement)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow: Where Content Actually Lives

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ðŸ—„ï¸  DATA LAYER (Pantry)                           â”‚
â”‚                                                                              â”‚
â”‚  These are the ACTUAL STORAGE systems - where content lives:                â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  QDRANT (Vector Database)                                            â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Stores: Document embeddings, chunk vectors                      â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Contains: Textbook content, code patterns, technical docs       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  NEO4J (Graph Database)                                              â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Stores: Relationships between concepts, cross-references        â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Contains: Bookâ†’Chapterâ†’Sectionâ†’Concept relationships           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  JSON FILES (Local Textbooks)                                        â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Stores: Raw textbook JSON files                                 â”‚   â”‚
â”‚  â”‚  â””â”€â†’ Location: /Users/kevintoles/POC/textbooks/JSON Texts/           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

The Semantic Search Service QUERIES these systems - it doesn't contain them.
```

---

## Multi-Model Coordination Flow

### Agent Conversation Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              User Query                                       â”‚
â”‚          "LLM code understanding with multi-stage chunking for RAG"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ORCHESTRATOR STATE MACHINE                            â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STATE 1: GENERATION                                                      â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚ CodeT5+ Agent                                                        â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Input:  "Extract technical search terms for: LLM code understanding  â”‚ â”‚ â”‚
â”‚  â”‚ â”‚          with multi-stage chunking for RAG"                          â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Output: {                                                             â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   "primary_terms": ["chunking", "RAG", "embedding", "LLM"],          â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   "related_terms": ["tokenization", "vector", "retrieval"],          â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   "code_patterns": ["text_splitter", "chunk_size", "overlap"]        â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ }                                                                     â”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STATE 2: VALIDATION                                                      â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚ GraphCodeBERT Agent                                                  â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Input:  Generated terms + Original query + Domain context            â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Validation Rules:                                                     â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   âœ“ "chunking" - Valid (LLM context, not memory allocation)          â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   âœ“ "RAG" - Valid (retrieval augmented generation)                   â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   âœ“ "embedding" - Valid (vector representations)                     â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   âœ— "split" - Rejected (too generic, high false positive rate)       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Expansions Added:                                                     â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   + "semantic_search" (related to RAG)                               â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   + "context_window" (related to chunking)                           â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   + "HNSW" (related to vector indexing)                              â”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STATE 3: RANKING                                                         â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚ â”‚ CodeBERT Agent                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Input:  Validated terms + Original query embedding                   â”‚ â”‚ â”‚
â”‚  â”‚ â”‚                                                                       â”‚ â”‚ â”‚
â”‚  â”‚ â”‚ Similarity Scoring:                                                   â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   1. chunking         â†’ 0.95 (highest relevance)                     â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   2. RAG              â†’ 0.92                                         â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   3. embedding        â†’ 0.89                                         â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   4. context_window   â†’ 0.85                                         â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   5. semantic_search  â†’ 0.82                                         â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   6. tokenization     â†’ 0.78                                         â”‚ â”‚ â”‚
â”‚  â”‚ â”‚   7. vector           â†’ 0.75                                         â”‚ â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ STATE 4: CONSENSUS                                                       â”‚ â”‚
â”‚  â”‚                                                                           â”‚ â”‚
â”‚  â”‚ Agreement Filter: Terms must be approved by â‰¥2 models                    â”‚ â”‚
â”‚  â”‚                                                                           â”‚ â”‚
â”‚  â”‚ Final Output:                                                             â”‚ â”‚
â”‚  â”‚ {                                                                         â”‚ â”‚
â”‚  â”‚   "search_terms": [                                                       â”‚ â”‚
â”‚  â”‚     {"term": "chunking", "score": 0.95, "models_agreed": 3},             â”‚ â”‚
â”‚  â”‚     {"term": "RAG", "score": 0.92, "models_agreed": 3},                  â”‚ â”‚
â”‚  â”‚     {"term": "embedding", "score": 0.89, "models_agreed": 3},            â”‚ â”‚
â”‚  â”‚     {"term": "context_window", "score": 0.85, "models_agreed": 2},       â”‚ â”‚
â”‚  â”‚     {"term": "semantic_search", "score": 0.82, "models_agreed": 2}       â”‚ â”‚
â”‚  â”‚   ],                                                                      â”‚ â”‚
â”‚  â”‚   "excluded_terms": [                                                     â”‚ â”‚
â”‚  â”‚     {"term": "split", "reason": "Too generic", "models_agreed": 1}       â”‚ â”‚
â”‚  â”‚   ]                                                                       â”‚ â”‚
â”‚  â”‚ }                                                                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model Selection Rationale

### The Trio: Why These Three Models?

| Model | Role | Strength | HuggingFace ID |
|-------|------|----------|----------------|
| **CodeT5+** | Generator | Encoder-decoder architecture enables text generation; trained on NLâ†”Code pairs | `Salesforce/codet5p-220m` |
| **GraphCodeBERT** | Validator | Understands code structure via data flow graphs; catches semantic mismatches | `microsoft/graphcodebert-base` |
| **CodeBERT** | Ranker | Fast embeddings for similarity scoring; well-established baseline | `microsoft/codebert-base` |

### Model Comparison Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Capability     â”‚    CodeT5+     â”‚ GraphCodeBERT  â”‚    CodeBERT    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Text Generation    â”‚       âœ…       â”‚       âŒ       â”‚       âŒ       â”‚
â”‚ Code Structure     â”‚       âš ï¸       â”‚       âœ…       â”‚       âš ï¸       â”‚
â”‚ Embeddings         â”‚       âœ…       â”‚       âœ…       â”‚       âœ…       â”‚
â”‚ Zero-shot Ready    â”‚       âœ…       â”‚       âš ï¸       â”‚       âš ï¸       â”‚
â”‚ Parameters         â”‚    220M-6B     â”‚     125M       â”‚     125M       â”‚
â”‚ Inference Speed    â”‚    Medium      â”‚     Fast       â”‚     Fast       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend: âœ… Excellent  âš ï¸ Partial  âŒ Not supported
```

---

## Service API Design

### REST Endpoints

```yaml
openapi: 3.0.0
info:
  title: Code Understanding Orchestrator API
  version: 1.0.0

paths:
  /api/v1/extract:
    post:
      summary: Extract search terms from query
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                query:
                  type: string
                  example: "LLM code understanding with multi-stage chunking"
                domain:
                  type: string
                  example: "ai-ml"
                options:
                  type: object
                  properties:
                    min_confidence:
                      type: number
                      default: 0.7
                    max_terms:
                      type: integer
                      default: 10
                    require_consensus:
                      type: boolean
                      default: true
      responses:
        '200':
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExtractionResult'

  /api/v1/validate:
    post:
      summary: Validate terms against domain context
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                terms:
                  type: array
                  items:
                    type: string
                query:
                  type: string
                domain:
                  type: string

  /v1/search:
    post:
      summary: Full pipeline - extract, validate, and search
      description: Combines extraction with semantic search service

components:
  schemas:
    ExtractionResult:
      type: object
      properties:
        search_terms:
          type: array
          items:
            type: object
            properties:
              term:
                type: string
              score:
                type: number
              models_agreed:
                type: integer
        excluded_terms:
          type: array
        metadata:
          type: object
          properties:
            processing_time_ms:
              type: integer
            models_used:
              type: array
```

---

## Repository Structure

```
code-understanding-orchestrator/
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI application entry
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py           # /extract endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ validate.py          # /validate endpoint
â”‚   â”‚   â”‚   â””â”€â”€ search.py            # /search endpoint
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ requests.py
â”‚   â”‚       â””â”€â”€ responses.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                  # BaseAgent abstract class
â”‚   â”‚   â”œâ”€â”€ codet5_agent.py          # CodeT5+ Generator
â”‚   â”‚   â”œâ”€â”€ graphcodebert_agent.py   # GraphCodeBERT Validator
â”‚   â”‚   â””â”€â”€ codebert_agent.py        # CodeBERT Ranker
â”‚   â”‚
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state_machine.py         # LangGraph state definitions
â”‚   â”‚   â”œâ”€â”€ graph.py                 # Orchestration graph
â”‚   â”‚   â””â”€â”€ consensus.py             # Multi-model agreement logic
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py              # Model loading/caching
â”‚   â”‚   â””â”€â”€ inference.py             # Inference utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ semantic_search.py       # Downstream service client
â”‚   â”‚
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ settings.py              # Pydantic settings
â”‚       â””â”€â”€ models.yaml              # Model configurations
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”‚   â”œâ”€â”€ test_orchestrator.py
â”‚   â”‚   â””â”€â”€ test_consensus.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_full_pipeline.py
â”‚   â””â”€â”€ fixtures/
â”‚       â””â”€â”€ sample_queries.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_models.py           # Pre-download HF models
â”‚   â””â”€â”€ benchmark.py                 # Performance benchmarking
â”‚
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ configmap.yaml
â”‚   â””â”€â”€ helm/
â”‚       â””â”€â”€ code-understanding-orchestrator/
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ API.md
    â”œâ”€â”€ ARCHITECTURE.md
    â””â”€â”€ DEPLOYMENT.md
```

---

## Core Implementation

### State Machine Definition (LangGraph)

```python
# src/orchestrator/state_machine.py
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END

class OrchestratorState(TypedDict):
    """State shared across all agents."""
    query: str
    domain: str
    
    # Generator output
    generated_terms: list[str]
    related_terms: list[str]
    code_patterns: list[str]
    
    # Validator output
    validated_terms: list[str]
    rejected_terms: list[dict]
    expanded_terms: list[str]
    
    # Ranker output
    ranked_terms: list[dict]
    
    # Final output
    final_terms: list[dict]
    excluded_terms: list[dict]
    
    # Metadata
    processing_steps: list[str]
    errors: list[str]


def create_orchestrator_graph() -> StateGraph:
    """Create the multi-model orchestration graph."""
    
    graph = StateGraph(OrchestratorState)
    
    # Add nodes
    graph.add_node("generate", generate_terms)
    graph.add_node("validate", validate_terms)
    graph.add_node("rank", rank_terms)
    graph.add_node("consensus", build_consensus)
    
    # Define edges
    graph.set_entry_point("generate")
    graph.add_edge("generate", "validate")
    graph.add_edge("validate", "rank")
    graph.add_edge("rank", "consensus")
    graph.add_edge("consensus", END)
    
    return graph.compile()
```

### Agent Base Class

```python
# src/agents/base.py
from abc import ABC, abstractmethod
from typing import Any

class BaseCodeAgent(ABC):
    """Base class for code understanding agents."""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
    
    @abstractmethod
    def load_model(self) -> None:
        """Load the model from HuggingFace."""
        pass
    
    @abstractmethod
    def process(self, state: dict) -> dict:
        """Process state and return updated state."""
        pass
    
    def health_check(self) -> bool:
        """Verify model is loaded and functional."""
        return self.model is not None
```

### CodeT5+ Generator Agent

```python
# src/agents/codet5_agent.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from .base import BaseCodeAgent

class CodeT5PlusAgent(BaseCodeAgent):
    """Generator agent using CodeT5+ for term extraction."""
    
    def __init__(self, model_size: str = "220m"):
        model_name = f"Salesforce/codet5p-{model_size}"
        super().__init__(model_name)
    
    def load_model(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
        self.model.to(self.device)
    
    def process(self, state: dict) -> dict:
        """Extract search terms from query."""
        query = state["query"]
        domain = state.get("domain", "general")
        
        prompt = f"""Extract technical search terms and concepts from this query.
Domain: {domain}
Query: {query}

Output format: term1, term2, term3, ..."""
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model.generate(
            **inputs,
            max_length=100,
            num_beams=5,
            early_stopping=True
        )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        terms = [t.strip() for t in result.split(",")]
        
        state["generated_terms"] = terms
        state["processing_steps"].append("CodeT5+ generation complete")
        
        return state
```

### GraphCodeBERT Validator Agent

```python
# src/agents/graphcodebert_agent.py
from transformers import AutoTokenizer, AutoModel
import torch
from .base import BaseCodeAgent

class GraphCodeBERTAgent(BaseCodeAgent):
    """Validator agent using GraphCodeBERT for semantic validation."""
    
    def __init__(self):
        super().__init__("microsoft/graphcodebert-base")
        self.domain_embeddings = {}  # Cached domain concept embeddings
    
    def load_model(self) -> None:
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)
        self.model.to(self.device)
    
    def get_embedding(self, text: str) -> torch.Tensor:
        """Get embedding for text."""
        inputs = self.tokenizer(text, return_tensors="pt", 
                                truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Use [CLS] token embedding
            return outputs.last_hidden_state[:, 0, :]
    
    def process(self, state: dict) -> dict:
        """Validate terms against query context."""
        query = state["query"]
        terms = state["generated_terms"]
        
        query_embedding = self.get_embedding(query)
        
        validated = []
        rejected = []
        
        for term in terms:
            term_embedding = self.get_embedding(term)
            similarity = torch.cosine_similarity(query_embedding, term_embedding)
            
            if similarity > 0.5:  # Threshold
                validated.append(term)
            else:
                rejected.append({
                    "term": term,
                    "reason": "Low semantic similarity to query",
                    "score": similarity.item()
                })
        
        # Expand with related terms
        expanded = self._expand_terms(query_embedding, validated)
        
        state["validated_terms"] = validated
        state["rejected_terms"] = rejected
        state["expanded_terms"] = expanded
        state["processing_steps"].append("GraphCodeBERT validation complete")
        
        return state
    
    def _expand_terms(self, query_embedding: torch.Tensor, 
                      validated: list[str]) -> list[str]:
        """Expand with semantically related terms."""
        # Implementation: compare against domain concept bank
        expansions = []
        domain_concepts = [
            "semantic_search", "context_window", "tokenizer",
            "HNSW", "vector_store", "retrieval_augmented"
        ]
        
        for concept in domain_concepts:
            if concept not in validated:
                concept_embedding = self.get_embedding(concept)
                similarity = torch.cosine_similarity(
                    query_embedding, concept_embedding
                )
                if similarity > 0.6:
                    expansions.append(concept)
        
        return expansions
```

---

## Deployment Architecture

### Docker Compose (Development)

```yaml
# docker-compose.yml
version: '3.8'

services:
  orchestrator:
    build: .
    ports:
      - "8080:8080"
    environment:
      - MODEL_CACHE_DIR=/models
      - DEVICE=cpu
      - LOG_LEVEL=INFO
    volumes:
      - model-cache:/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional: GPU-enabled inference
  orchestrator-gpu:
    build: .
    runtime: nvidia
    environment:
      - DEVICE=cuda
      - MODEL_CACHE_DIR=/models
    volumes:
      - model-cache:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  model-cache:
```

### Kubernetes Deployment

```yaml
# deploy/kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: code-understanding-orchestrator
  labels:
    app: code-understanding-orchestrator
spec:
  replicas: 2
  selector:
    matchLabels:
      app: code-understanding-orchestrator
  template:
    metadata:
      labels:
        app: code-understanding-orchestrator
    spec:
      containers:
        - name: orchestrator
          image: code-understanding-orchestrator:latest
          ports:
            - containerPort: 8080
          resources:
            requests:
              memory: "4Gi"
              cpu: "2"
            limits:
              memory: "8Gi"
              cpu: "4"
          env:
            - name: MODEL_CACHE_DIR
              value: "/models"
            - name: SEMANTIC_SEARCH_URL
              valueFrom:
                configMapKeyRef:
                  name: orchestrator-config
                  key: semantic_search_url
          volumeMounts:
            - name: model-cache
              mountPath: /models
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache-pvc
```

---

## Integration Points

### With Semantic Search Service

```python
# src/clients/semantic_search.py
import httpx
from typing import Optional

class SemanticSearchClient:
    """Client for semantic-search-service integration."""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=30.0)
    
    async def search(
        self,
        terms: list[str],
        taxonomy: Optional[str] = None,
        limit: int = 20
    ) -> dict:
        """Execute search with extracted terms."""
        response = await self.client.post(
            f"{self.base_url}/v1/search",
            json={
                "query_terms": terms,
                "taxonomy_filter": taxonomy,
                "limit": limit,
                "search_type": "hybrid"  # vector + keyword
            }
        )
        return response.json()
```

### With AI Agents Service

```python
# Example integration in ai-agents
from code_understanding_client import CodeUnderstandingClient

class CrossReferenceAgent:
    def __init__(self):
        self.orchestrator = CodeUnderstandingClient(
            url="http://code-understanding-orchestrator:8080"
        )
        self.search = SemanticSearchClient(...)
    
    async def find_references(self, query: str, taxonomy: str) -> list:
        # Step 1: Extract terms via orchestrator
        extraction = await self.orchestrator.extract(
            query=query,
            domain="ai-ml",
            options={"min_confidence": 0.7}
        )
        
        # Step 2: Search with extracted terms
        terms = [t["term"] for t in extraction["search_terms"]]
        results = await self.search.search(terms, taxonomy=taxonomy)
        
        return results
```

---

## Use Cases Beyond Cross-References

This service is designed for reuse across multiple applications:

| Use Case | Description |
|----------|-------------|
| **Code Search** | Extract search terms from natural language queries about code |
| **Documentation Retrieval** | Find relevant docs based on technical questions |
| **API Discovery** | Match user intent to available API endpoints |
| **Codebase Q&A** | Power RAG systems for code understanding |
| **Technical Support** | Route support tickets to relevant knowledge base articles |
| **Code Review** | Identify related code patterns and best practices |

---

## Performance Considerations

### Model Loading Strategy

```python
# Lazy loading with caching
class ModelRegistry:
    _instances = {}
    
    @classmethod
    def get_model(cls, model_type: str):
        if model_type not in cls._instances:
            if model_type == "codet5":
                cls._instances[model_type] = CodeT5PlusAgent()
            elif model_type == "graphcodebert":
                cls._instances[model_type] = GraphCodeBERTAgent()
            elif model_type == "codebert":
                cls._instances[model_type] = CodeBERTAgent()
            cls._instances[model_type].load_model()
        return cls._instances[model_type]
```

### Batch Processing

```python
# For high-throughput scenarios
async def batch_extract(queries: list[str]) -> list[dict]:
    """Process multiple queries in parallel."""
    tasks = [extract_single(q) for q in queries]
    return await asyncio.gather(*tasks)
```

### Caching Layer

```python
# Redis caching for repeated queries
@cache(ttl=3600)  # 1 hour
async def extract_terms(query: str, domain: str) -> dict:
    return await orchestrator.process(query, domain)
```

---

## Monitoring & Observability

### Metrics to Track

| Metric | Description |
|--------|-------------|
| `extraction_duration_seconds` | Time to extract terms |
| `model_inference_duration_seconds` | Per-model inference time |
| `terms_generated_total` | Number of terms generated |
| `terms_validated_ratio` | Ratio of validated vs rejected terms |
| `consensus_agreement_rate` | How often models agree |
| `cache_hit_rate` | Effectiveness of caching |

### Structured Logging

```python
logger.info(
    "Extraction complete",
    extra={
        "query_hash": hash(query),
        "terms_count": len(final_terms),
        "models_used": ["codet5", "graphcodebert", "codebert"],
        "processing_time_ms": elapsed,
        "consensus_rate": agreement_rate
    }
)
```

---

## Next Steps

1. **Phase 1**: Create repository and basic FastAPI structure
2. **Phase 2**: Implement CodeT5+ generator agent
3. **Phase 3**: Add GraphCodeBERT validator agent
4. **Phase 4**: Add CodeBERT ranker agent
5. **Phase 5**: Implement LangGraph orchestration
6. **Phase 6**: Integration tests with semantic-search-service
7. **Phase 7**: Docker/Kubernetes deployment
8. **Phase 8**: Performance optimization and caching

---

## References

- [CodeT5+ Paper](https://arxiv.org/abs/2305.07922)
- [GraphCodeBERT Paper](https://arxiv.org/abs/2009.08366)
- [CodeBERT Paper](https://arxiv.org/abs/2002.08155)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
